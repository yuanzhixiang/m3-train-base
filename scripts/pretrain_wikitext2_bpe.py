#!/usr/bin/env python3
# scripts/pretrain_wikitext2_bpe.py
"""
Milestone 3: GPT-2 BPE (tiktoken) + WikiText-2 (raw) pretraining
No Hugging Face dependency.

What you get:
- data download (with mirror + fallback)
- GPT-2 BPE tokenization + caching
- smoke tests: forward / causality / backward / param delta
- optional: overfit one fixed batch (correctness seal)
- train + val evaluation + checkpoint + sampling
"""

from __future__ import annotations

import argparse
import math
import os
import shutil
import time
import zipfile
from pathlib import Path
from typing import Dict, Tuple

import torch

try:
    import tiktoken
except ImportError as e:
    raise SystemExit(
        "tiktoken is required for this script.\n"
        "Install: pip install -U tiktoken\n"
    ) from e

# Reuse your minimal GPT model implementation
from minimal_gpt import GPT


# -----------------------------
# Utils
# -----------------------------

def pick_device(device_arg: str | None) -> torch.device:
    if device_arg:
        return torch.device(device_arg)

    if torch.cuda.is_available():
        return torch.device("cuda")
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def download(url: str, dest: Path, timeout: int = 60) -> None:
    """Download URL to dest (atomic write)."""
    if dest.exists() and dest.stat().st_size > 0:
        return

    dest.parent.mkdir(parents=True, exist_ok=True)
    tmp = dest.with_suffix(dest.suffix + ".tmp")

    print(f"[download] {url}")
    import urllib.request

    try:
        with urllib.request.urlopen(url, timeout=timeout) as r, open(tmp, "wb") as f:
            shutil.copyfileobj(r, f)
        tmp.replace(dest)
    except Exception:
        if tmp.exists():
            tmp.unlink()
        raise


def extract_zip(zip_path: Path, out_dir: Path) -> None:
    out_dir.mkdir(parents=True, exist_ok=True)
    with zipfile.ZipFile(zip_path, "r") as zf:
        zf.extractall(out_dir)


def find_wikitext_files(root: Path) -> Dict[str, Path]:
    """
    Return dict with keys train/valid/test pointing to raw files.
    We accept either raw or tokens files.
    """
    patterns = [
        ("train", ["wiki.train.raw", "wiki.train.tokens"]),
        ("valid", ["wiki.valid.raw", "wiki.valid.tokens"]),
        ("test",  ["wiki.test.raw",  "wiki.test.tokens"]),
    ]

    found: Dict[str, Path] = {}
    for split, names in patterns:
        for name in names:
            hits = list(root.rglob(name))
            if hits:
                found[split] = hits[0]
                break

    missing = [k for k in ["train", "valid", "test"] if k not in found]
    if missing:
        raise FileNotFoundError(f"Cannot find files for splits: {missing} under {root}")

    return found


def ensure_wikitext2_raw(data_dir: Path) -> Dict[str, Path]:
    """
    Prefer Smerity mirror zip (raw). If that fails, fallback to a public raw-file mirror.
    """
    data_dir.mkdir(parents=True, exist_ok=True)

    # 1) Try Smerity zip mirror (raw)
    zip_url = "https://wikitext.smerity.com/wikitext-2-raw-v1.zip"
    zip_path = data_dir / "wikitext-2-raw-v1.zip"

    extracted_marker = data_dir / ".extracted_ok"

    if not extracted_marker.exists():
        try:
            download(zip_url, zip_path)
            extract_zip(zip_path, data_dir)
            extracted_marker.write_text("ok")
        except Exception as e:
            print(f"[warn] failed to download/extract smerity zip: {e}")
            print("[warn] falling back to raw files mirror (no zip)")

            # 2) Fallback: direct raw files
            # (This mirror hosts wiki.train.raw/wiki.valid.raw/wiki.test.raw)
            base = "https://cosmo.zip/pub/datasets/wikitext-2-raw"
            for split in ["train", "valid", "test"]:
                url = f"{base}/wiki.{split}.raw"
                dest = data_dir / f"wiki.{split}.raw"
                download(url, dest)

            extracted_marker.write_text("ok_fallback")

    files = find_wikitext_files(data_dir)
    print("[data] files:")
    for k, p in files.items():
        print(f"  - {k}: {p}")
    return files


def encode_text(enc, text: str) -> torch.Tensor:
    # Add end-of-text token once at the end (optional but helpful as a boundary)
    eot = enc.encode("<|endoftext|>", allowed_special={"<|endoftext|>"})[0]
    ids = enc.encode(text) + [eot]
    return torch.tensor(ids, dtype=torch.int32)


def prepare_tokens(
    enc,
    files: Dict[str, Path],
    cache_dir: Path,
    force_rebuild: bool = False,
) -> Dict[str, torch.Tensor]:
    """
    Tokenize train/valid/test and cache as torch tensors on disk.
    """
    cache_dir.mkdir(parents=True, exist_ok=True)
    out: Dict[str, torch.Tensor] = {}

    for split, path in files.items():
        cache_path = cache_dir / f"{split}.pt"
        if cache_path.exists() and not force_rebuild:
            data = torch.load(cache_path, map_location="cpu")
            out[split] = data
            continue

        print(f"[tokenize] split={split} reading {path.name}")
        text = path.read_text(encoding="utf-8", errors="replace")
        t0 = time.time()
        data = encode_text(enc, text)
        dt = time.time() - t0
        torch.save(data, cache_path)
        print(f"[tokenize] split={split} tokens={data.numel():,} saved={cache_path} ({dt:.2f}s)")
        out[split] = data

    return out


def get_batch(data_1d: torch.Tensor, block_size: int, batch_size: int, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    data_1d: 1D tensor of token ids (int32 or int64)
    returns x,y of shape (B,T) in int64 on device
    """
    n = data_1d.size(0)
    if n <= block_size + 1:
        raise ValueError(f"Dataset too small: n={n}, block_size={block_size}")

    ix = torch.randint(0, n - block_size - 1, (batch_size,))
    x = torch.stack([data_1d[i:i + block_size] for i in ix]).long().to(device)
    y = torch.stack([data_1d[i + 1:i + block_size + 1] for i in ix]).long().to(device)
    return x, y


@torch.no_grad()
def estimate_loss(
    model: GPT,
    train_data: torch.Tensor,
    val_data: torch.Tensor,
    block_size: int,
    batch_size: int,
    device: torch.device,
    eval_iters: int,
) -> Dict[str, float]:
    model.eval()
    out = {}
    for split, data in [("train", train_data), ("val", val_data)]:
        losses = []
        for _ in range(eval_iters):
            xb, yb = get_batch(data, block_size, batch_size, device)
            _, loss = model(xb, yb)
            losses.append(loss.item())
        out[split] = float(sum(losses) / len(losses))
    model.train()
    return out


def configure_optimizer(model: torch.nn.Module, lr: float, weight_decay: float) -> torch.optim.Optimizer:
    """
    AdamW with a typical decoupled weight decay pattern:
    - decay for 2D weights (matmul weights)
    - no decay for biases and LayerNorm/Embedding weights
    """
    decay_params = []
    no_decay_params = []
    for name, p in model.named_parameters():
        if not p.requires_grad:
            continue
        if p.dim() >= 2 and (".ln" not in name) and ("ln_" not in name) and (not name.endswith(".bias")):
            decay_params.append(p)
        else:
            no_decay_params.append(p)

    optim_groups = [
        {"params": decay_params, "weight_decay": weight_decay},
        {"params": no_decay_params, "weight_decay": 0.0},
    ]
    optimizer = torch.optim.AdamW(optim_groups, lr=lr, betas=(0.9, 0.95), eps=1e-8)
    return optimizer


def get_lr(step: int, *, base_lr: float, min_lr: float, warmup_steps: int, max_steps: int) -> float:
    if max_steps <= 0:
        return base_lr
    if step < warmup_steps:
        return base_lr * (step + 1) / max(1, warmup_steps)
    if step >= max_steps:
        return min_lr
    decay_ratio = (step - warmup_steps) / max(1, (max_steps - warmup_steps))
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (base_lr - min_lr)


def save_checkpoint(
    path: Path,
    model: GPT,
    optimizer: torch.optim.Optimizer,
    step: int,
    config: dict,
) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    ckpt = {
        "step": step,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "config": config,
    }
    torch.save(ckpt, path)


# -----------------------------
# Main
# -----------------------------

def main():
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨ï¼Œç”¨äºé…ç½®è®­ç»ƒå‚æ•°
    parser = argparse.ArgumentParser(description="Pretrain minimal GPT on WikiText-2 (raw) with GPT-2 BPE (tiktoken).")
    # å®éªŒè¿è¡Œåç§°ï¼Œç”¨äºä¿å­˜æ£€æŸ¥ç‚¹å’Œæ—¥å¿—
    parser.add_argument("--run_name", type=str, default="wt2_bpe")
    # éšæœºç§å­ï¼Œç”¨äºå¯å¤ç°æ€§
    parser.add_argument("--seed", type=int, default=1337)
    # è®¾å¤‡é€‰æ‹©ï¼šcuda(NVIDIA GPU) / mps(Apple Silicon) / cpuï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
    parser.add_argument("--device", type=str, default=None, help="cuda|mps|cpu (default: auto)")

    # Model æ¨¡å‹æ¶æ„å‚æ•°
    # Transformer å±‚æ•°ï¼ˆæ¯å±‚åŒ…å«ä¸€ä¸ªè‡ªæ³¨æ„åŠ›å—å’Œä¸€ä¸ªå‰é¦ˆç½‘ç»œï¼‰
    parser.add_argument("--n_layer", type=int, default=4)
    # å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
    parser.add_argument("--n_head", type=int, default=4)
    # åµŒå…¥ç»´åº¦ï¼ˆæ¯ä¸ª token çš„å‘é‡è¡¨ç¤ºç»´åº¦ï¼‰
    parser.add_argument("--n_embd", type=int, default=256)
    # ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆæ¨¡å‹ä¸€æ¬¡èƒ½å¤„ç†çš„æœ€å¤§ token æ•°ï¼‰
    parser.add_argument("--block_size", type=int, default=128)
    # Dropout æ¯”ä¾‹ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
    parser.add_argument("--dropout", type=float, default=0.1)

    # Training è®­ç»ƒå‚æ•°
    # å¾®æ‰¹æ¬¡å¤§å°ï¼ˆæ¯æ¬¡å‰å‘ä¼ æ’­å¤„ç†çš„æ ·æœ¬æ•°ï¼Œä¿æŒè¾ƒå°ä»¥èŠ‚çœæ˜¾å­˜ï¼Œå› ä¸º 50k è¯æ±‡è¡¨çš„ logits å¾ˆå¤§ï¼‰
    parser.add_argument("--batch_size", type=int, default=4, help="micro-batch size (keep small; logits is huge with 50k vocab)")
    # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡ï¼šæœ‰æ•ˆæ‰¹æ¬¡ = batch_size * grad_accumï¼‰
    parser.add_argument("--grad_accum", type=int, default=8, help="gradient accumulation steps")
    # æœ€å¤§è®­ç»ƒæ­¥æ•°
    parser.add_argument("--max_steps", type=int, default=2000)
    # åŸºç¡€å­¦ä¹ ç‡ï¼ˆå³°å€¼å­¦ä¹ ç‡ï¼‰
    parser.add_argument("--lr", type=float, default=3e-4)
    # æœ€å°å­¦ä¹ ç‡ï¼ˆä½™å¼¦é€€ç«åçš„æœ€ç»ˆå­¦ä¹ ç‡ï¼‰
    parser.add_argument("--min_lr", type=float, default=3e-5)
    # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°ï¼ˆä» 0 çº¿æ€§å¢åŠ åˆ° base_lrï¼‰
    parser.add_argument("--warmup_steps", type=int, default=100)
    # æƒé‡è¡°å‡ç³»æ•°ï¼ˆL2 æ­£åˆ™åŒ–ï¼ŒAdamW ä¸­çš„è§£è€¦æƒé‡è¡°å‡ï¼‰
    parser.add_argument("--weight_decay", type=float, default=0.1)
    # æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
    parser.add_argument("--grad_clip", type=float, default=1.0)

    # Eval / logging è¯„ä¼°å’Œæ—¥å¿—è®°å½•
    # è®­ç»ƒæ—¥å¿—æ‰“å°é—´éš”ï¼ˆæ¯ N æ­¥æ‰“å°ä¸€æ¬¡è®­ç»ƒ lossï¼‰
    parser.add_argument("--log_interval", type=int, default=50)
    # è¯„ä¼°é—´éš”ï¼ˆæ¯ N æ­¥åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šè¯„ä¼°ä¸€æ¬¡ï¼‰
    parser.add_argument("--eval_interval", type=int, default=200)
    # è¯„ä¼°æ—¶çš„è¿­ä»£æ¬¡æ•°ï¼ˆè®¡ç®—å¹³å‡ loss çš„æ‰¹æ¬¡æ•°ï¼‰
    parser.add_argument("--eval_iters", type=int, default=25)

    # Correctness seal æ­£ç¡®æ€§éªŒè¯
    # å¦‚æœ >0ï¼šåœ¨å›ºå®šæ‰¹æ¬¡ä¸Šè¿‡æ‹Ÿåˆ N æ­¥ï¼ˆç”¨äºéªŒè¯æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸å­¦ä¹ ï¼‰
    parser.add_argument("--overfit_one_batch_steps", type=int, default=0, help="if >0: overfit a fixed batch for N steps")

    # Data æ•°æ®ç›¸å…³
    # æ•°æ®é›†ç›®å½•ï¼ŒNone è¡¨ç¤ºä½¿ç”¨é»˜è®¤è·¯å¾„
    parser.add_argument("--data_dir", type=str, default=None)
    # æ˜¯å¦å¼ºåˆ¶é‡æ–° tokenizeï¼ˆå¦åˆ™ä¼šä½¿ç”¨ç¼“å­˜ï¼‰
    parser.add_argument("--force_rebuild_tokens", action="store_true")

    # Sampling ç”Ÿæˆé‡‡æ ·å‚æ•°
    # ç”Ÿæˆæ–‡æœ¬æ—¶çš„æç¤ºè¯ï¼ˆpromptï¼‰
    parser.add_argument("--sample_prompt", type=str, default="The history of")
    # ç”Ÿæˆçš„ token æ•°é‡
    parser.add_argument("--sample_tokens", type=int, default=120)
    # é‡‡æ ·æ¸©åº¦ï¼ˆè¶Šé«˜è¶Šéšæœºï¼Œè¶Šä½è¶Šç¡®å®šæ€§ï¼‰
    parser.add_argument("--temperature", type=float, default=0.9)
    # Top-K é‡‡æ ·ï¼ˆåªä»æ¦‚ç‡æœ€é«˜çš„ K ä¸ª token ä¸­é‡‡æ ·ï¼‰
    parser.add_argument("--top_k", type=int, default=40)

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°
    torch.manual_seed(args.seed)

    # æ ¹æ®å‚æ•°æˆ–è‡ªåŠ¨æ£€æµ‹é€‰æ‹©è®¡ç®—è®¾å¤‡ï¼ˆGPU/CPUï¼‰
    device = pick_device(args.device)
    print(f"[device] {device}")

    # Project root: repo_root/scripts/this_file.py -> repo_root
    # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå½“å‰æ–‡ä»¶åœ¨ scripts/ ä¸‹ï¼Œçˆ¶ç›®å½•çš„çˆ¶ç›®å½•æ˜¯é¡¹ç›®æ ¹ç›®å½•ï¼‰
    repo_root = Path(__file__).resolve().parent.parent
    # åˆ›å»ºè¿è¡Œç›®å½•ï¼Œç”¨äºä¿å­˜æ£€æŸ¥ç‚¹å’Œæ—¥å¿—
    run_dir = repo_root / "runs" / args.run_name
    run_dir.mkdir(parents=True, exist_ok=True)

    # è®¾ç½®æ•°æ®ç›®å½•ï¼šå¦‚æœæŒ‡å®šäº† data_dir å‚æ•°åˆ™ä½¿ç”¨å®ƒï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    data_dir = Path(args.data_dir) if args.data_dir else (repo_root / "data" / "wikitext2_raw")
    # tokenize åçš„ç¼“å­˜ç›®å½•
    cache_dir = repo_root / "data" / "cache_wt2_gpt2_bpe"

    # Tokenizer åˆå§‹åŒ– GPT-2 çš„ BPE tokenizer
    enc = tiktoken.get_encoding("gpt2")
    # GPT-2 çš„è¯æ±‡è¡¨å¤§å°åº”è¯¥æ˜¯ 50257
    vocab_size = enc.n_vocab  # should be 50257 for GPT-2
    print(f"[tokenizer] gpt2_bpe vocab_size={vocab_size}")

    # quick tokenizer sanity å¿«é€ŸéªŒè¯ tokenizer æ˜¯å¦æ­£å¸¸å·¥ä½œ
    # æµ‹è¯•ä¸åŒç±»å‹çš„å­—ç¬¦ä¸²ï¼šè½¬ä¹‰å­—ç¬¦ã€è‹±æ–‡ã€ä¸­æ–‡ã€emoji
    test_strings = ["GPT\\", "Hello world!", "ä¸­æ–‡ä¹Ÿå¯ä»¥", "emojiğŸ™‚ test"]
    for s in test_strings:
        # ç¼–ç ï¼šæ–‡æœ¬ -> token IDs
        ids = enc.encode(s)
        # è§£ç ï¼štoken IDs -> æ–‡æœ¬
        s2 = enc.decode(ids)
        # éªŒè¯å¾€è¿”è½¬æ¢æ˜¯å¦ä¸€è‡´
        print(f"[tok_test] {s!r} -> {len(ids)} tokens -> roundtrip_ok={s2 == s}")

    # Data ä¸‹è½½å¹¶å‡†å¤‡æ•°æ®
    # ç¡®ä¿ WikiText-2 æ•°æ®é›†å·²ä¸‹è½½ï¼ˆå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨ä¸‹è½½ï¼‰
    files = ensure_wikitext2_raw(data_dir)
    # å¯¹æ–‡æœ¬è¿›è¡Œ tokenize å¹¶ç¼“å­˜ä¸º .pt æ–‡ä»¶ï¼ˆå¦‚æœç¼“å­˜å­˜åœ¨åˆ™ç›´æ¥åŠ è½½ï¼‰
    tokens = prepare_tokens(enc, files, cache_dir, force_rebuild=args.force_rebuild_tokens)

    # è·å–è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ token åºåˆ—
    train_data = tokens["train"]
    val_data = tokens["valid"]

    # æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    print(f"[data] train_tokens={train_data.numel():,} val_tokens={val_data.numel():,}")

    # Baseline check åŸºçº¿æ£€æŸ¥
    # éšæœºçŒœæµ‹çš„ loss æ˜¯ ln(vocab_size)ï¼Œæ¨¡å‹çš„ loss åº”è¯¥ä½äºè¿™ä¸ªå€¼
    baseline = math.log(vocab_size)
    print(f"[baseline] ln(vocab_size)={baseline:.4f}")

    # Model åˆ›å»ºæ¨¡å‹
    model = GPT(
        vocab_size=vocab_size,        # è¯æ±‡è¡¨å¤§å°
        block_size=args.block_size,   # ä¸Šä¸‹æ–‡é•¿åº¦
        n_layer=args.n_layer,         # Transformer å±‚æ•°
        n_head=args.n_head,           # æ³¨æ„åŠ›å¤´æ•°
        n_embd=args.n_embd,           # åµŒå…¥ç»´åº¦
        dropout=args.dropout,         # Dropout æ¯”ä¾‹
    ).to(device)  # å°†æ¨¡å‹ç§»åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰

    # è®¡ç®—æ¨¡å‹å‚æ•°æ€»æ•°
    n_params = sum(p.numel() for p in model.parameters())
    print(f"[model] params={n_params/1e6:.3f}M")

    # é…ç½®ä¼˜åŒ–å™¨ï¼ˆAdamWï¼Œå¸¦æƒé‡è¡°å‡ï¼‰
    optimizer = configure_optimizer(model, lr=args.lr, weight_decay=args.weight_decay)

    # -----------------------------
    # Smoke tests: forward / causality / backward / param delta
    # å†’çƒŸæµ‹è¯•ï¼šéªŒè¯æ¨¡å‹çš„åŸºæœ¬åŠŸèƒ½æ˜¯å¦æ­£å¸¸
    # -----------------------------
    # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨ dropout ç­‰ï¼‰
    model.train()
    # ä»è®­ç»ƒé›†è·å–ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®
    xb, yb = get_batch(train_data, args.block_size, args.batch_size, device)
    # å‰å‘ä¼ æ’­ï¼šè®¡ç®— logits å’Œ loss
    logits, loss = model(xb, yb)
    print(f"[smoke.forward] logits={tuple(logits.shape)} loss={loss.item():.4f}")

    # causality test in eval mode (so dropout won't affect)
    # å› æœæ€§æµ‹è¯•ï¼šéªŒè¯æ¨¡å‹æ˜¯å¦æ»¡è¶³å› æœçº¦æŸï¼ˆtoken åªä¾èµ–è¿‡å»ï¼Œä¸ä¾èµ–æœªæ¥ï¼‰
    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
        model.eval()  # è¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­ dropoutï¼‰
        # å–æ‰¹æ¬¡ä¸­çš„ç¬¬ä¸€ä¸ªæ ·æœ¬
        test = xb[:1].clone()
        # è®¡ç®—åŸå§‹è¾“å…¥çš„ logits
        logits1, _ = model(test)
        # ä¿®æ”¹æœ€åä¸€ä¸ª token
        test2 = test.clone()
        test2[0, -1] = (test2[0, -1] + 1) % vocab_size
        # è®¡ç®—ä¿®æ”¹åçš„ logits
        logits2, _ = model(test2)
        # æ¯”è¾ƒå‰é¢ä½ç½®çš„ logits å·®å¼‚ï¼ˆåº”è¯¥ä¸º 0ï¼Œå› ä¸ºå®ƒä»¬ä¸åº”è¯¥ä¾èµ–æœ€åä¸€ä¸ª tokenï¼‰
        diff = (logits1[:, :-1, :] - logits2[:, :-1, :]).abs().max().item()
        print(f"[smoke.causality] max_diff_on_past_positions={diff:.6f} (should be ~0)")
        model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼

    # åå‘ä¼ æ’­æµ‹è¯•ï¼šéªŒè¯æ¢¯åº¦è®¡ç®—æ˜¯å¦æ­£å¸¸
    # æ¸…ç©ºä¼˜åŒ–å™¨ä¸­çš„æ¢¯åº¦
    optimizer.zero_grad(set_to_none=True)
    # å‰å‘ä¼ æ’­è®¡ç®— loss
    _, loss = model(xb, yb)
    # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
    loss.backward()

    # è®¡ç®—æ¢¯åº¦çš„ L2 èŒƒæ•°ï¼ˆç”¨äºæ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸ï¼‰
    grad_norm = 0.0
    for p in model.parameters():
        if p.grad is not None:
            grad_norm += p.grad.data.norm(2).item() ** 2
    grad_norm = grad_norm ** 0.5
    print(f"[smoke.backward] grad_norm={grad_norm:.4f}")

    # å‚æ•°æ›´æ–°æµ‹è¯•ï¼šéªŒè¯ä¼˜åŒ–å™¨æ˜¯å¦æ­£å¸¸æ›´æ–°å‚æ•°
    # è®°å½•æ›´æ–°å‰çš„å‚æ•°
    before = torch.nn.utils.parameters_to_vector(model.parameters()).detach().clone()
    # æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–ï¼ˆæ›´æ–°å‚æ•°ï¼‰
    optimizer.step()
    # è®°å½•æ›´æ–°åçš„å‚æ•°
    after = torch.nn.utils.parameters_to_vector(model.parameters()).detach()
    # è®¡ç®—å‚æ•°å˜åŒ–çš„èŒƒæ•°ï¼ˆåº”è¯¥ >0ï¼Œè¯´æ˜å‚æ•°ç¡®å®è¢«æ›´æ–°äº†ï¼‰
    print(f"[smoke.update] param_delta_norm={(after - before).norm().item():.6f}")

    # -----------------------------
    # Optional: overfit one fixed batch (correctness seal)
    # å¯é€‰ï¼šåœ¨å›ºå®šæ‰¹æ¬¡ä¸Šè¿‡æ‹Ÿåˆï¼ˆæ­£ç¡®æ€§éªŒè¯ï¼‰
    # -----------------------------
    # å¦‚æœè®¾ç½®äº† overfit_one_batch_steps > 0ï¼Œåˆ™æ‰§è¡Œè¿‡æ‹Ÿåˆæµ‹è¯•
    if args.overfit_one_batch_steps > 0:
        print(f"\n[overfit-one-batch] steps={args.overfit_one_batch_steps}")
        model.train()
        # è·å–ä¸€ä¸ªå›ºå®šçš„æ‰¹æ¬¡ï¼ˆä¸å˜çš„æ•°æ®ï¼‰
        xb_fix, yb_fix = get_batch(train_data, args.block_size, args.batch_size, device)
        # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨
        optimizer = configure_optimizer(model, lr=args.lr, weight_decay=args.weight_decay)

        # åœ¨å›ºå®šæ‰¹æ¬¡ä¸Šåå¤è®­ç»ƒï¼ˆå¦‚æœæ¨¡å‹èƒ½å­¦ä¹ ï¼Œloss åº”è¯¥å¿«é€Ÿä¸‹é™åˆ°æ¥è¿‘ 0ï¼‰
        for i in range(args.overfit_one_batch_steps):
            # å‰å‘ä¼ æ’­
            _, l = model(xb_fix, yb_fix)
            # æ¸…ç©ºæ¢¯åº¦
            optimizer.zero_grad(set_to_none=True)
            # åå‘ä¼ æ’­
            l.backward()
            # å¦‚æœè®¾ç½®äº†æ¢¯åº¦è£å‰ªï¼Œåˆ™è£å‰ªæ¢¯åº¦
            if args.grad_clip > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
            # æ›´æ–°å‚æ•°
            optimizer.step()
            # æ¯ 50 æ­¥æˆ–ç¬¬ä¸€æ­¥æ‰“å° loss
            if (i + 1) % 50 == 0 or i == 0:
                print(f"[overfit-one-batch] step {i+1} loss={l.item():.6f}")

    # -----------------------------
    # Train
    # æ­£å¼è®­ç»ƒå¾ªç¯
    # -----------------------------
    # å¦‚æœ max_steps <= 0ï¼Œåˆ™è·³è¿‡è®­ç»ƒï¼Œåªæ‰§è¡Œå†’çƒŸæµ‹è¯•åé€€å‡º
    if args.max_steps <= 0:
        print("\n[done] max_steps<=0, exit after smoke tests.")
        return

    print(f"\n[train] max_steps={args.max_steps} batch_size={args.batch_size} grad_accum={args.grad_accum} block_size={args.block_size}")
    model.train()
    # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼ˆå¦‚æœä¹‹å‰è¿›è¡Œäº† overfit æµ‹è¯•ï¼Œéœ€è¦é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€ï¼‰
    optimizer = configure_optimizer(model, lr=args.lr, weight_decay=args.weight_decay)

    # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
    t0 = time.time()
    # ä¸»è®­ç»ƒå¾ªç¯
    for step in range(args.max_steps):
        # LR schedule å­¦ä¹ ç‡è°ƒåº¦
        # æ ¹æ®å½“å‰æ­¥æ•°è®¡ç®—å­¦ä¹ ç‡ï¼ˆwarmup + cosine decayï¼‰
        lr = get_lr(
            step,
            base_lr=args.lr,
            min_lr=args.min_lr,
            warmup_steps=args.warmup_steps,
            max_steps=args.max_steps,
        )
        # æ›´æ–°ä¼˜åŒ–å™¨ä¸­æ‰€æœ‰å‚æ•°ç»„çš„å­¦ä¹ ç‡
        for pg in optimizer.param_groups:
            pg["lr"] = lr

        # Eval + checkpoint + sample è¯„ä¼° + ä¿å­˜æ£€æŸ¥ç‚¹ + ç”Ÿæˆæ ·æœ¬
        # æ¯éš” eval_interval æ­¥æˆ–æœ€åä¸€æ­¥æ‰§è¡Œè¯„ä¼°
        if step % args.eval_interval == 0 or step == args.max_steps - 1:
            # åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
            losses = estimate_loss(
                model,
                train_data=train_data,
                val_data=val_data,
                block_size=args.block_size,
                batch_size=args.batch_size,
                device=device,
                eval_iters=args.eval_iters,
            )
            # è®¡ç®—å·²è®­ç»ƒæ—¶é—´
            elapsed = time.time() - t0
            print(f"\n[eval] step={step} lr={lr:.2e} train_loss={losses['train']:.4f} val_loss={losses['val']:.4f} elapsed={elapsed:.1f}s")

            # sample ç”Ÿæˆæ–‡æœ¬æ ·æœ¬
            with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
                model.eval()  # è¯„ä¼°æ¨¡å¼
                # å°†æç¤ºè¯ç¼–ç ä¸º token IDs
                prompt_ids = enc.encode(args.sample_prompt)
                # è½¬æ¢ä¸º tensor å¹¶ç§»åˆ°è®¾å¤‡ä¸Š
                idx = torch.tensor([prompt_ids], dtype=torch.long, device=device)
                # ç”Ÿæˆæ–‡æœ¬
                out = model.generate(idx, max_new_tokens=args.sample_tokens, temperature=args.temperature, top_k=args.top_k)
                # è§£ç ç”Ÿæˆçš„ token IDs ä¸ºæ–‡æœ¬
                text = enc.decode(out[0].tolist())
                model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼
            print("[sample]")
            print(text)

            # save ä¿å­˜æ£€æŸ¥ç‚¹
            ckpt_path = run_dir / "ckpt_latest.pt"
            # æ„é€ é…ç½®å­—å…¸ï¼ˆåŒ…å«æ‰€æœ‰è¶…å‚æ•°ï¼‰
            config = vars(args).copy()
            config.update({
                "vocab_size": vocab_size,
                "encoding": "gpt2",
            })
            # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
            save_checkpoint(ckpt_path, model, optimizer, step=step, config=config)
            # also save a step checkpoint occasionally å¶å°”ä¿å­˜ç‰¹å®šæ­¥æ•°çš„æ£€æŸ¥ç‚¹
            if step % (args.eval_interval * 5) == 0:
                save_checkpoint(run_dir / f"ckpt_step{step:06d}.pt", model, optimizer, step=step, config=config)
            print(f"[ckpt] saved: {ckpt_path}")

        # gradient accumulation æ¢¯åº¦ç´¯ç§¯
        # æ¸…ç©ºæ¢¯åº¦
        optimizer.zero_grad(set_to_none=True)
        # ç´¯ç§¯çš„ loss
        loss_accum = 0.0

        # æ‰§è¡Œå¤šä¸ªå¾®æ‰¹æ¬¡çš„æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡ï¼‰
        for micro in range(args.grad_accum):
            # è·å–ä¸€ä¸ªå¾®æ‰¹æ¬¡
            xb, yb = get_batch(train_data, args.block_size, args.batch_size, device)
            # å‰å‘ä¼ æ’­
            _, loss = model(xb, yb)
            # å°† loss é™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼ˆç›¸å½“äºæ±‚å¹³å‡ï¼‰
            loss = loss / args.grad_accum
            # åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ä¼šç´¯ç§¯ï¼‰
            loss.backward()
            # ç´¯åŠ  loss ç”¨äºæ—¥å¿—
            loss_accum += loss.item()

        # å¦‚æœè®¾ç½®äº†æ¢¯åº¦è£å‰ªï¼Œåˆ™è£å‰ªæ¢¯åº¦ï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        if args.grad_clip > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)

        # æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–ï¼ˆæ›´æ–°æ‰€æœ‰å‚æ•°ï¼‰
        optimizer.step()

        # æ¯éš” log_interval æ­¥æ‰“å°è®­ç»ƒæ—¥å¿—
        if step % args.log_interval == 0:
            print(f"[train] step={step} lr={lr:.2e} loss={loss_accum:.4f}")

    print("\n[done] training finished.")


if __name__ == "__main__":
    main()
